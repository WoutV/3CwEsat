\documentclass[a4paper, 12pt, titlepage]{report}

%Taal: Nederlands ("Inhoudsopgave", "Hoofdstuk",...)
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{algorithmic}
\usepackage{amsmath, amssymb, textcomp, mathtools}

%Geen nummering bij secties en hoofdstukkden
\setcounter{secnumdepth}{-1} 

%geen indents
\setlength\parindent{0pt}


%"Figuur" in vet
\makeatletter
\renewcommand{\fnum@figure}{\small\textbf{\figurename~\thefigure}}
\makeatother

\usepackage[dutch]{babel}
\begin{document}

\title{\textbf{Numerieke Modellering en Benadering}\\\textit{Practicum 1: Eigenwaardenproblemen}\\}
\author{De Wolf Peter\\ Vekemans Wout}

\date{\today}
\begin{titlepage}
	\maketitle
	\thispagestyle{empty}
\end{titlepage}

\newpage

\listoffigures

\newpage

\section{Inleiding}
In dit practicum onderzoeken we methoden voor het bepalen van eigenwaarden van vole matrices. In een eerste sectie beschouwen we enkele theoretische eigenschappen van de methoden. In een tweede sectie worden de convergentie-eigenschappen van da methoden onderzocht aan de hand van MATLAB-experimenten. In de derde en laatste sectie gaan we dieper in op \'e\'en van de methoden, namelijk de Jacobi-methode.
\section{Theoretische eigenschappen}


\subsection{Opgave 1}
Gelijktijdige iteratie is een andere naam voor het toepassen van de methode van de machten op meerdere kolommen van $A$ tegelijk. Zo kunnen de grootste $n$ eigenwaarden van de matrix $A$ gevonden worden.  Om met gelijktijdige iteratie de kleinste $n$ eigenwaarden van A te berekenen is het voldoende om het algoritme toe te passen op $A^{-1}$ in plaats van op $A$. Dit is makkelijk in te zien. De eigenwaarden van de inverse van een singuliere matrix $A$ zijn gelijk aan het omgekeerde van de eigenwaarden van $A$.\\
\begin{subequations} \label{eq:simIt}
\begin{align}
 Ax  = \lambda x \\
A\frac{1}{\lambda}x =  x\\
A^{-1}A\frac{1}{\lambda}x  =  A^{-1}x\\
A^{-1}x  =  \frac{1}{\lambda}x
\end{align}
\end{subequations}

Als we dit algoritme uitschrijven krijgen we volgende pseudocode:\\
\begin{algorithmic}
 \STATE Initialiseer $\hat{Q}^{(0)} \in \mathbb{R}^{m \times n}$ met $n$ orthonormale kolommen.
\FOR	 {$k = 1,2,...$}
\STATE $Z = A^{-1}\hat{Q}^{k-1}$
\STATE $\hat{Q}^{k}\hat{R}^{k} = Z$
\ENDFOR
\end{algorithmic}
Dit zal convergeren naar de $n$ grootste eigenwaarden van $A^{-1}$, die zoals eerder aangetoond in \eqref{eq:simIt} overeenkomen met de $n$ kleinste eigenwaarden van $A$.

\subsection{Opgave 2}
\paragraph{a)} 
De Rayleigh quoti\"ent iteratie convergeert zeer snel naar de juiste waarde van de eigenwaarden van $A$. Elke iteratiestap verdrievoudigt het aantal juiste cijfers. Elke stap dient er wel een stelsel te worden opgelost dat meer en meer singulier wordt naarmate de benadering voor de eigenwaarde juister wordt. Het betreft het stelsel
\begin{equation}\label{eq:rayleighstelsel}
 (A=\lambda^{(k-1)}I)w = v^{(k-1)}
\end{equation}
Als $\lambda$ dichter en dichter bij een eigenwaarde van $A$ komt wordt $A-\lambda^{(k-1)}I$ meer en meer singulier, en zal de inverse van deze matrix, die nodig is om \eqref{eq:rayleighstelsel} op te lossen, ervoor zorgen dat de vector $w$ enorm wordt opgeblazen. De vector zal echter wel in de juiste richting blijven wijzen, waardoor dit geen problemen oplevert.\\
\paragraph{b)}
Gegeven een matrix $A \in \mathbb{R}^{m \times m}$ en een vector $x \in \mathbb{R}^{m \times 1}$, waarbij $x$ een benadering is voor een eigenvector van $A$. Toon aan dat de oplossing $\rho \in \mathbb{R}$ van het minimalisatieprobleem 
\begin{equation}
 \underset{\rho \in \mathbb{R}}{\text{min}}|| Ax-\rho x||_{2}
\end{equation}
overeenkomt met het Rayleighquoti\"ent van $x$.\\
Dit probleem lijkt op het standaard kleinste kwadraten probleem $Ax \approx b$. We proberen nu een $\rho$ te vinden zodat $||Ax-\rho x||_{2}$ geminimaliseerd wordt. De overeenkomstige normaalvergelijkingen (cf $A^TAx = A^Tb)$ worden gegeven door 
\begin{subequations}
\begin{align}
  x^Tx\rho = x^TAx\\
 \rho = \frac{x^TAx}{x^Tx}=r(x)
\end{align}
\end{subequations}
waarbij $r(x)$ het Rayleigh quoti\"ent van $x$ voorstelt.

\subsection{Opgave 3}
\paragraph{a)}
Gegeven een schatting voor de eigenvector horend bij de derde kleinste eigenwaarde van een symmetrische matrix $A \in \mathbb{R}^{m \times m}$ , bepaal de bijhorende eigenwaarde.\\
Als een benadering $v$ voor een eigenvector gekend is, kan er m.b.v. het Rayleigh quoti\"ent $\mu$ van die vector een benadering van de bijhorende eigenwaarde worden berekend.
\begin{equation}
	\mu = \frac{v^TAv}{v^Tv}
	\label{eqn:rayleighquotient}
\end{equation}
Daarna kan er met een inverse iteratiestap een betere benadering voor de eigenvector worden berekend. Als deze stappen $k$ keer herhaald worden spreekt men van Rayleigh quoti\"ent iteratie. Deze methode kent een kubische convergentie voor een symmetrische matrix. Het is wel niet 100 procent gegarandeerd dat er effectief convergentie is richting de eigenwaarde horende bij de vector. \\
Om het algoritme effici\"ent te kunnen uitvoeren moet de matrix eerst gereduceerd worden tot Hessenberg vorm, dit vraagt $\mathcal{O}(\frac{10}{3}m^3)$ flops. Na deze reductie heeft Rayleigh quoti\"ent iteratie maar $\mathcal{O}(m)$ flops per iteratiestap nodig, in tegenstelling tot de $\mathcal{O}(m^3)$ flops per stap indien er niet gereduceerd wordt. 
\paragraph{b)}
Gegeven een symmetrische matrix $A \in \mathbb{R}^{m \times m}$, bepaal de eigenwaarde het dichts gelegen bij een getal $\alpha$.\\
Om een schatting van een eigenvector te krijgen, maken we gebruik van 1 iteratiestap van inverse iteratie. Hiervoor lossen we het stelsel \eqref{eq:rayleighstelsel} op, met $\alpha$ als waarde voor $\lambda$, en $v$ een willekeurige vector met $||v||_2 = 1$.\\ 
De vector $w$ die we dan krijgen kunnen we gebruiken als startvector voor Rayleigh quoti\"ent iteratie. Dit heeft kubische convergentie en is dus het snelst van alle beschouwde algoritmes. Zoals hierboven vermeld is er eerst reductie tot Hessenberg vorm nodig om het aantal flops per iteratiestap te verminderen. 
\section{Convergentie-experimenten}
Voor al deze experimenten wordt gebruik gemaakt van een volle, re\"ele symmetrische matrix A. We laden hiervoor de gegeven matrix $mat1.txt$ in in MATLAB.

\subsection{Opgave 4}
Het uitvoeren van het $spy$-commando laat zien dat er geen enkel $non-zero$ element in de matrix zit. De uitvoering van het QR-algoritme op een matrix met die afmetingen (nog steeds relatief klein) zou zeer veel werk vragen. \\
Na het reduceren tot Hessenberg vorm zien we dat alle elementen onder de eerste benedendiagonaal nul zijn geworden. Verdere analyse leert ons dat ook de elementen boven de eerste bovendiagonaal allemaal van grootteorde $\epsilon_{mach}$ zijn. Als we de Hessenberg vorm van de matrix afronden tot op 15 decimalen verkijgen we een tridiagonale matrix.(zie figuur \ref{hessenberg}) Dit komt doordat de originele matrix symmetrisch was. De reductie naar Hessenberg vorm zorgt ervoor dat het algoritme niet op een volledige matrix moet inwerken. Het uitvoeren van het QR-algoritme vraagt nu slechts $\mathcal{O}(n^2)$ flops. 
\begin{figure}[!h]x
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\textwidth]{structuur_hessenberg.eps}
\subcaption{zonder afronden}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\textwidth]{afgerond.eps}
\subcaption{met afronden}
\end{subfigure}

\caption{Structuur van de Hessenberg matrix}
\label{hessenberg}
\end{figure}

\subsection{Opgave 5}
Het toepassen van de drie verschillende versies van het QR algoritme geeft ons figuur \ref{3QRs}. . Als maat voor de convergentie gebruiken we de waarde van een subdiagonaal elementje van $A$. Dit doen we omdat de matrix convergeert naar een diagonaalmatrix. Hoe dichter die waarde bij nul ligt, hoe dichter de matrix een diagonaalmatrix benadert. We zien duidelijk dat er 1 methode trager is dan de rest. We zien op de grafiek duidelijk dat het residu bij het algoritme zonder shifts lineair convergeert naar 0. De algoritmes met shifts (Rayleigh en Wilkinson) convergeren kubisch en zijn dus veel sneller dan het algoritme zonder shifts.

\begin{figure}[htb]
	\centering
	\includegraphics[width=\textwidth]{3QRS.eps}
	\caption{convergentie van verschillende QR algoritmes}
	\label{3QRs}
\end{figure}


Voor het algoritme zonder shifts stemt dit volledig overeen met de theorie. Theorem 28.4 (p. 218 in het handboek) geeft duidelijk aan dat bij het gebruik van QR zonder shifts $A^{(k)}$ lineair convergeert naar een diagonaalmatrix met de eigenwaarden van $A$ als elementen.In Tabel \ref{errors} staan de relatieve fouten op de grootste eigenwaarde.\\
\begin{center}
\begin{table}[h]

\begin{tabular}{r||r|r|r}

Iteratie	&	unshifted	&	Wilkinson	&	Rayleigh	\\
\hline
1	&	7.6359E-01	&	1.8360E-02	&	1.8347E-02	\\
2	&	2.1011E-01	&	9.8332E-07	&	1.2342E-05	\\
3	&	8.0838E-02	&	1.1383E-18	&	9.5580E-15	\\
4	&	3.3388E-02	&	1.1131E-50	&	8.9132E-42	\\
5	&	1.4090E-02	&	1.1243E-114	&	7.2306E-97	\\

\end{tabular}
\caption{Residu van de 3 verschillende methoden, voor de eerste vijf stappen}
\label{errorNoShift}
\end{table}
\end{center}

\begin{center}
 \begin{table}[h]
  \begin{tabular}{r||r|r|r|r|r}
  Iteratie & 23&22&21&20&19\\
  \hline
5	&	2.7954E-02	&	1.1750E-01	&	1.0582E-01	&	2.4273E-01	&	2.3798E-01	\\
10	&	1.4943E-02	&	2.0071E-02	&	8.4508E-02	&	8.2623E-02	&	1.0694E-01	\\
15	&	9.6654E-03	&	3.1292E-03	&	9.0310E-02	&	4.8149E-02	&	2.6853E-02	\\
20	&	6.1540E-03	&	3.7826E-03	&	3.4726E-02	&	4.6832E-02	&	2.3820E-02	\\
25	&	3.8228E-03	&	2.6950E-03	&	1.3951E-02	&	4.8232E-02	&	1.2391E-02	\\
30	&	2.3320E-03	&	1.7462E-03	&	7.0906E-03	&	1.6654E-02	&	3.8615E-02	\\
35	&	1.4057E-03	&	1.0889E-03	&	3.9605E-03	&	3.2062E-03	&	4.6617E-02	\\
40	&	8.4102E-04	&	6.6583E-04	&	2.2457E-03	&	1.7235E-04	&	3.9414E-02	\\
45	&	5.0085E-04	&	4.0271E-04	&	1.2673E-03	&	6.4671E-04	&	2.6736E-02	\\
50	&	2.9744E-04	&	2.4205E-04	&	7.1057E-04	&	5.1331E-04	&	2.6268E-02	\\
55	&	1.7635E-04	&	1.4495E-04	&	3.9652E-04	&	3.2931E-04	&	2.3048E-02	\\
60	&	1.0445E-04	&	8.6613E-05	&	2.2060E-04	&	1.9614E-04	&	1.4949E-02	\\
65	&	6.1824E-05	&	5.1682E-05	&	1.2251E-04	&	1.1316E-04	&	8.8874E-03	\\
70	&	3.6582E-05	&	3.0811E-05	&	6.7957E-05	&	6.4321E-05	&	4.9702E-03	\\
75	&	2.1642E-05	&	1.8357E-05	&	3.7671E-05	&	3.6294E-05	&	2.6765E-03	\\
80	&	1.2801E-05	&	1.0932E-05	&	2.0873E-05	&	2.0403E-05	&	1.4106E-03	\\
85	&	7.5716E-06	&	6.5079E-06	&	1.1561E-05	&	1.1446E-05	&	7.3479E-04	\\
90	&	4.4782E-06	&	3.8731E-06	&	6.4022E-06	&	6.4144E-06	&	3.8037E-04	\\
95	&	2.6485E-06	&	2.3044E-06	&	3.5445E-06	&	3.5920E-06	&	1.9627E-04	\\



  \end{tabular}
\caption{Relatieve fout op vijf grootste berekende eigenwaarden (Rayleigh shift)}
\label{errorRayleigh}
 \end{table}

\end{center}


\begin{center}
 \begin{table}[h]
  \begin{tabular}{r||r|r|r|r|r}
  Iteratie & 23&22&21&20&19\\
  \hline
  5	&	3.5241E-02	&	1.7599E-01	&	1.5860E-01	&	2.9753E-01	&	2.7171E-01	\\
10	&	1.6399E-02	&	3.5473E-02	&	1.0074E-01	&	7.4094E-02	&	1.4469E-01	\\
15	&	1.0546E-02	&	1.9560E-03	&	8.6089E-02	&	5.8539E-02	&	3.5689E-02	\\
20	&	6.7499E-03	&	3.9381E-03	&	4.2712E-02	&	4.9227E-02	&	2.3114E-02	\\
25	&	4.2117E-03	&	2.9159E-03	&	1.6377E-02	&	5.5393E-02	&	6.6471E-03	\\
30	&	2.5772E-03	&	1.9121E-03	&	8.0228E-03	&	2.1555E-02	&	3.4651E-02	\\
35	&	1.5566E-03	&	1.1991E-03	&	4.4386E-03	&	4.7371E-03	&	4.2576E-02	\\
40	&	9.3243E-04	&	7.3546E-04	&	2.5159E-03	&	1.3738E-04	&	4.3532E-02	\\
45	&	5.5571E-04	&	4.4559E-04	&	1.4217E-03	&	6.3519E-04	&	2.8144E-02	\\
50	&	3.3017E-04	&	2.6809E-04	&	7.9809E-04	&	5.5136E-04	&	2.5672E-02	\\
55	&	1.9580E-04	&	1.6064E-04	&	4.4572E-04	&	3.6282E-04	&	2.4825E-02	\\
60	&	1.1599E-04	&	9.6021E-05	&	2.4810E-04	&	2.1829E-04	&	1.6428E-02	\\
65	&	6.8662E-05	&	5.7309E-05	&	1.3782E-04	&	1.2651E-04	&	9.9183E-03	\\
70	&	4.0631E-05	&	3.4171E-05	&	7.6463E-05	&	7.2069E-05	&	5.6034E-03	\\
75	&	2.4038E-05	&	2.0361E-05	&	4.2390E-05	&	4.0710E-05	&	3.0357E-03	\\
80	&	1.4219E-05	&	1.2126E-05	&	2.3489E-05	&	2.2898E-05	&	1.6053E-03	\\
85	&	8.4102E-06	&	7.2194E-06	&	1.3012E-05	&	1.2851E-05	&	8.3769E-04	\\
90	&	4.9742E-06	&	4.2968E-06	&	7.2056E-06	&	7.2026E-06	&	4.3405E-04	\\
95	&	2.9419E-06	&	2.5566E-06	&	3.9895E-06	&	4.0338E-06	&	2.2408E-04	\\


  \end{tabular}
\caption{Relatieve fout op vijf grootste berekende eigenwaarden (Wilkinson)}
\label{errorWilkinson}
 \end{table}

\end{center}




\subsection{Opgave 6}
De Rayleigh quoti\"ent iteratie en de Rayleigh shift methode zouden allebei kubisch moeten convergeren. In figuur \ref{vglRayleigh} zijn onze numerieke bevindingen weergegeven. We zien duidelijk dat er een gelijkaardige convergentiesnelheid is. Dit komt doordat de Rayleigh shift methode gebruik maakt van het Rayleigh quoti\"ent. Het QR algoritme met Rayleigh shifts is namelijk een methode waarbij er snelle convergentie is in de laatste kolom van $Q^{(k)}$. Als we \eqref{eqn:rayleighquotient} toepassen op de laatste kolom van de matrix krijgen we een goede schatting voor de eigenwaarde horende bij die laatste kolom. 

\begin{equation}
	\mu^{(k)} = \frac{(q^{(k)}_m)^TAq^{(k)}_m}{(q^{(k)}_m)^Tq^{(k)}_m} = (q^{(k)}_m)^TAq^{(k)}_m
	\label{eqn:rayleighquotient}
\end{equation}

Nu is dit getal gelijk aan $A^{(k)}_{mm}$ en kan dit getal zo als shift gebruikt worden voor de volgende iteratiestap.\\

De convergentie van gelijktijdige iteratie en QR zonder shifts wordt ge\"illustreerd in figuur \ref{vglSimQR}. Als residu gebruiken we een elementje net onder de diagonaal van $A$. Aangezien de matrix moet convergeren naar een diagonaalmatrix geeft dit een duidelijk beeld van de convergentie. Hoe snelle het elementje naar 0 nadert, hoe sneller de convergentie. Het is duidelijk dat deze methodes ongeveer even snel convergeren. Dit kan eenvoudig worden aangetoond. Het algoritme voor gelijktijdige iteratie is als volgt:

\begin{algorithmic}
 \STATE Kies $\hat{Q}^{(0)}\in \mathbb{R}^{m\times n}$ met orthonormale kolommen
 \FOR{$k=1,2,...$} 
 	\STATE $Z = A\hat{Q}^{(k-1)}$
 	\STATE $\hat{Q}^{(k)}\hat{R}^{(k)} = Z$
 \ENDFOR
\end{algorithmic}

Theorema 28.3 (p. 216) in het handboek toont aan dat gelijktijdige iteratie en unshifted QR dezelfde matrices $\underline{R}^{(k)},\underline{Q}^{(k)} en \underline{A}^{(k)}$ genereren. Het is ook eenvoudig in te zien dat dit de matrices zijn die gedefinieerd worden door de $k^{de}$ macht van de QR factorisatie van $A$.\\

Volgens theorema 28.4 (p. 218) convergeert het QR algoritme zonder shifts lineair naar een diagonaalmatrix. Aangezien gelijktijdige iteratie dezelfde matrices produceert moet deze methode een gelijkaardige convergentiesnelheid hebben. Uit figuur \ref{vglSimQR} wordt duidelijk dat matrix $A$ inderdaar lineair convergeert naar een diagonaalmatrix.


\begin{figure}[htb]
	\centering
	\includegraphics[width=0.8\textwidth]{vergelijkingLineair.eps}
	\caption{Vergelijking convergentie van unshifted QR en gelijktijdig iteratie}
	\label{vglSimQR}
\end{figure}


\subsection{Opgave 7}
We hebben een random ijle matrix $A \in \mathbb{R}^{1000\times1000}$ gegenereerd. Als we hierop de Arnoldi iteratie toepassen en de Ritz waarden berekenen voor elke iteratiestap krijgen we figuur \ref{arnoldiGraph}. We zien duidelijk dat er zeer snelle, geometrische convergentie is naar de extreme eigenwaarde 24.3868 die we berekend hebben met het commando $eigs(A)$. De andere eigenwaarden convergeren niet zo snel. 
\begin{figure}[htb]
	\centering
	\includegraphics[width=\textwidth]{arnoldi.jpg}
	\caption{Convergentie van Arnoldi-Ritz waarden}
	\label{arnoldiGraph}
\end{figure}
\section{Alternatieve eigenwaardenalgoritmen}

\subsection{Opgave 8}

\subsection{Opgave 9}

\subsection{Opgave 10}

\end{document}
